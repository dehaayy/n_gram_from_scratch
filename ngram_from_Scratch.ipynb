{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835fd28b",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87d9c3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:41:27.839802Z",
     "start_time": "2024-02-11T18:41:27.832173Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the data\n",
    "def clean_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        documents = file.readlines()\n",
    "\n",
    "\n",
    "    cleaned_documents = []\n",
    "    for doc in documents:\n",
    "        doc = doc.lower()\n",
    "        # Remove all punctuation except \", . ?!\"\n",
    "        doc = re.sub(r\"[^\\w\\s,.?!]\", \"\", doc)\n",
    "        # Convert to lower case\n",
    "        # Add <START> and <END> tokens\n",
    "        doc = f\"START {doc.strip()} END\"\n",
    "        cleaned_documents.append(doc)\n",
    "    \n",
    "    return cleaned_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c23a1",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d797c177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:41:29.211883Z",
     "start_time": "2024-02-11T18:41:28.602566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the first text file\n",
    "gpt_path = '/Users/dehaay/Desktop/humvgpt 2/gpt.txt' \n",
    "gpt_text = clean_data(gpt_path)\n",
    "\n",
    "# Read the second text file\n",
    "human_path = '/Users/dehaay/Desktop/humvgpt 2/hum.txt'\n",
    "human_text = clean_data(human_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8e2c0f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:41:29.788996Z",
     "start_time": "2024-02-11T18:41:29.785463Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_parti(text,train_ratio):\n",
    "    split_index = int(len(text) * train_ratio)\n",
    "    train = text[:split_index]\n",
    "    test = text[split_index:]\n",
    "    print(len(train),len(test))\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c3a98",
   "metadata": {},
   "source": [
    "## Data partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85afb24d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:41:31.977409Z",
     "start_time": "2024-02-11T18:41:31.953619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18358 2040\n",
      "35741 3972\n"
     ]
    }
   ],
   "source": [
    "gpt_train,gpt_test = data_parti(gpt_text,0.9)\n",
    "\n",
    "human_train,human_test = data_parti(human_text,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50199c2",
   "metadata": {},
   "source": [
    "## Generate n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fe81c60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:41:33.312061Z",
     "start_time": "2024-02-11T18:41:33.304184Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Converts them [[word_array_of_paragrapgh1],[word_array_of_paragrapgh2]]\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize_paragraph(data):\n",
    "    \n",
    "    tokenized_paragraphs = []\n",
    "\n",
    "    # Iterate over each paragraph in data\n",
    "    for paragraph in data:\n",
    "        # Tokenize the paragraph by words using re.findall\n",
    "        # The pattern \\b\\w+\\b matches sequences of word characters that are bounded by word boundaries\n",
    "        tokens = re.findall(r'\\w+|[.]', paragraph)\n",
    "\n",
    "        # Append the list of tokens (words) for the current paragraph to tokenized_paragraphs\n",
    "        tokenized_paragraphs.append(tokens)\n",
    "        \n",
    "    return tokenized_paragraphs\n",
    "\n",
    "# Function to generate n-grams from tokenized paragraphs\n",
    "def generate_n_grams_from_paragraphs(tokenized_paragraphs, n):\n",
    "    n_grams_list = []\n",
    "    for paragraph_tokens in tokenized_paragraphs:\n",
    "        n_grams_list.extend(list(zip(*[paragraph_tokens[i:] for i in range(n)])))\n",
    "    return n_grams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb589d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:41:42.589437Z",
     "start_time": "2024-02-11T18:41:34.933872Z"
    }
   },
   "outputs": [],
   "source": [
    "gpt_train_tokenized = tokenize_paragraph(gpt_train)\n",
    "gpt_test_tokenized = tokenize_paragraph(gpt_test)\n",
    "\n",
    "human_train_tokenized = tokenize_paragraph(human_train)\n",
    "human_test_tokenized = tokenize_paragraph(human_test)\n",
    "\n",
    "# Generate bigrams and trigrams for the training set\n",
    "gpt_bigrams_train = Counter(generate_n_grams_from_paragraphs(gpt_train_tokenized, 2))\n",
    "gpt_trigrams_train = Counter(generate_n_grams_from_paragraphs(gpt_train_tokenized, 3))\n",
    "\n",
    "# Generate bigrams and trigrams for the test set\n",
    "gpt_bigrams_test = Counter(generate_n_grams_from_paragraphs(gpt_test_tokenized, 2))\n",
    "gpt_trigrams_test = Counter(generate_n_grams_from_paragraphs(gpt_test_tokenized, 3))\n",
    "\n",
    "# Generate bigrams and trigrams for the training set\n",
    "human_bigrams_train = Counter(generate_n_grams_from_paragraphs(human_train_tokenized, 2))\n",
    "human_trigrams_train = Counter(generate_n_grams_from_paragraphs(human_train_tokenized, 3))\n",
    "\n",
    "# Generate bigrams and trigrams for the test set\n",
    "human_bigrams_test = Counter(generate_n_grams_from_paragraphs(human_test_tokenized, 2))\n",
    "human_trigrams_test = Counter(generate_n_grams_from_paragraphs(human_test_tokenized, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee4035e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T02:25:29.014001Z",
     "start_time": "2024-02-11T02:25:25.763709Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_biagram_train = gpt_bigrams_train + human_bigrams_train\n",
    "combined_biagram_test = gpt_bigrams_test + human_bigrams_test\n",
    "\n",
    "combined_trigram_train = gpt_trigrams_train + human_trigrams_train\n",
    "combined_trigram_test = gpt_trigrams_test + human_trigrams_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7511922",
   "metadata": {},
   "source": [
    "## OOV Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da9c3d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T02:25:38.553744Z",
     "start_time": "2024-02-11T02:25:37.887738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total number of bigrams in the test set (counting repeats)\n",
    "total_bigrams_test = sum(combined_biagram_test.values())\n",
    "\n",
    "# Bigrams in the test set that are not in the training set\n",
    "oov_bigrams_test = {bigram: count for bigram, count in combined_biagram_test.items() if bigram not in combined_biagram_train}\n",
    "\n",
    "# Number of OOV bigrams in the test set (counting repeats)\n",
    "oov_bigrams_count = sum(oov_bigrams_test.values())\n",
    "\n",
    "# Calculate the OOV rate for bigrams\n",
    "oov_rate_bigrams = (oov_bigrams_count / total_bigrams_test) * 100\n",
    "\n",
    "\n",
    "# Total number of trigrams in the test set (counting repeats)\n",
    "total_trigrams_test = sum(combined_trigram_test.values())\n",
    "\n",
    "# Trigrams in the test set that are not in the training set\n",
    "oov_trigrams_test = {trigram: count for trigram, count in combined_trigram_test.items() if trigram not in combined_trigram_train}\n",
    "\n",
    "# Number of OOV trigrams in the test set (counting repeats)\n",
    "oov_trigrams_count = sum(oov_trigrams_test.values())\n",
    "\n",
    "# Calculate the OOV rate for trigrams\n",
    "oov_rate_trigrams = (oov_trigrams_count / total_trigrams_test) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6103ff38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T02:25:41.204286Z",
     "start_time": "2024-02-11T02:25:41.200789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV Rate for Bigrams: 10.72%\n",
      "OOV Rate for Trigrams: 37.89%\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOV Rate for Bigrams: {oov_rate_bigrams:.2f}%\")\n",
    "print(f\"OOV Rate for Trigrams: {oov_rate_trigrams:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7e782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfbfdfea",
   "metadata": {},
   "source": [
    "# Assess Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f32e553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:43:59.714428Z",
     "start_time": "2024-02-11T18:43:58.382852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bigram': 1116390, 'trigram': 3240641}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine n-gram frequencies into a structured format\n",
    "n_gram_models = {\n",
    "    'bigram': {\n",
    "        'human': human_bigrams_train,\n",
    "        'gpt': gpt_bigrams_train,\n",
    "    },\n",
    "    'trigram': {\n",
    "        'human': human_trigrams_train,\n",
    "        'gpt': gpt_trigrams_train,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Assume vocab_size is known (total unique bigrams and trigrams across both human and GPT training sets)\n",
    "vocab_size_bigrams = len(set(n_gram_models['bigram']['human']) | set(n_gram_models['bigram']['gpt']))\n",
    "vocab_size_trigrams = len(set(n_gram_models['trigram']['human']) | set(n_gram_models['trigram']['gpt']))\n",
    "\n",
    "vocab_size = {\n",
    "    'bigram': vocab_size_bigrams,\n",
    "    'trigram': vocab_size_trigrams\n",
    "}\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7aca13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T18:43:41.130354Z",
     "start_time": "2024-02-11T18:43:41.091232Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_gram_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (count_of_ngram \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m denominator\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Precompute counts before using the function\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m precomputed_counts \u001b[38;5;241m=\u001b[39m precompute_counts(n_gram_models)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_gram_models' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Pre-compute unigram and bigram counts for each label\n",
    "def precompute_counts(model_counts):\n",
    "    precomputed = {'bigram': {}, 'trigram': {}}\n",
    "    \n",
    "    for type_of_gram in model_counts:\n",
    "        for label in model_counts[type_of_gram]:\n",
    "            if type_of_gram == 'bigram':\n",
    "                # Precompute sum of counts for each unigram as the first word in a bigram\n",
    "                unigram_counts = defaultdict(int)\n",
    "                for bigram, count in model_counts[type_of_gram][label].items():\n",
    "                    unigram_counts[bigram[0]] += count\n",
    "                precomputed[type_of_gram][label] = unigram_counts\n",
    "            elif type_of_gram == 'trigram':\n",
    "                # Precompute sum of counts for each bigram as the first two words in a trigram\n",
    "                bigram_counts = defaultdict(int)\n",
    "                for trigram, count in model_counts[type_of_gram][label].items():\n",
    "                    bigram_counts[trigram[:2]] += count\n",
    "                precomputed[type_of_gram][label] = bigram_counts\n",
    "    \n",
    "    return precomputed\n",
    "\n",
    "# Modified function using precomputed sums\n",
    "def laplace_smoothed_probability(n_gram, label, type_of_gram, model_counts, precomputed_counts, vocab_size):\n",
    "    count_of_ngram = model_counts[type_of_gram][label].get(n_gram, 0)\n",
    "    \n",
    "    if type_of_gram == 'bigram':\n",
    "        unigram_count = precomputed_counts[type_of_gram][label].get(n_gram[0], 0)\n",
    "        denominator = unigram_count + vocab_size[type_of_gram]\n",
    "    elif type_of_gram == 'trigram':\n",
    "        bigram_part = n_gram[:2]\n",
    "        bigram_count = precomputed_counts[type_of_gram][label].get(bigram_part, 0)\n",
    "        denominator = bigram_count + vocab_size[type_of_gram]\n",
    "    else:\n",
    "        return \"wrong type\"\n",
    "    \n",
    "    return (count_of_ngram + 1) / denominator\n",
    "\n",
    "# Precompute counts before using the function\n",
    "precomputed_counts = precompute_counts(n_gram_models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "817a4834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T02:26:19.732157Z",
     "start_time": "2024-02-11T02:26:19.705705Z"
    }
   },
   "outputs": [],
   "source": [
    "total_bigrams_human = sum(human_bigrams_train.values())\n",
    "total_bigrams_gpt = sum(gpt_bigrams_train.values())\n",
    "\n",
    "# Calculate the overall total of bigrams\n",
    "total_bigrams = total_bigrams_human + total_bigrams_gpt\n",
    "\n",
    "# Calculate P(y) for each class\n",
    "P_y_human = total_bigrams_human / total_bigrams\n",
    "P_y_gpt = total_bigrams_gpt / total_bigrams\n",
    "\n",
    "def classify_document(document_n_grams, n_gram_type, n_gram_models, vocab_size):\n",
    "    \"\"\"\n",
    "    Classify a document based on its n-grams.\n",
    "\n",
    "    Parameters:\n",
    "    - document_n_grams: Counter object containing the document's n-grams.\n",
    "    - n_gram_type: 'bigram' or 'trigram' specifying the type of n-gram.\n",
    "    - n_gram_models: The structured n-gram models containing counts for human and GPT.\n",
    "    - vocab_size: The size of the vocabulary for the n-gram type.\n",
    "    - total_counts: Total counts of n-grams in the training data.\n",
    "\n",
    "    Returns:\n",
    "    - Classification label: 'human' or 'gpt'.\n",
    "    \"\"\"\n",
    "    # Initialize log probabilities to 0 (since we'll be summing logs)\n",
    "    log_prob_human = 0\n",
    "    log_prob_gpt = 0\n",
    "    \n",
    "\n",
    "    # Calculate log probabilities for the document under each model\n",
    "    for n_gram, count in document_n_grams.items():\n",
    "        prob_human = laplace_smoothed_probability(n_gram,'human',n_gram_type,n_gram_models,precomputed_counts,vocab_size)\n",
    "\n",
    "        prob_gpt =  laplace_smoothed_probability(n_gram,'gpt', n_gram_type,n_gram_models,precomputed_counts,vocab_size)\n",
    "        # Use log probabilities to avoid underflow\n",
    "        log_prob_human += math.log(prob_human) * count\n",
    "        log_prob_gpt += math.log(prob_gpt) * count\n",
    "\n",
    "    log_prob_human += math.log(P_y_human)\n",
    "    log_prob_gpt += math.log(P_y_gpt)\n",
    "    \n",
    "    return 'human' if log_prob_human > log_prob_gpt else 'gpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d60801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T02:26:23.156193Z",
     "start_time": "2024-02-11T02:26:20.347884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model Accuracy: 95.64%\n",
      "Trigram Model Accuracy: 93.00%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def evaluate_model(test_data, n_gram_type, n_gram_models, vocab_size):\n",
    "    correct = 0\n",
    "    #document : each tokenized paragrapgh\n",
    "    #true_label: classification of the document\n",
    "    for document, true_label in test_data:\n",
    "        \n",
    "        # Generate n-grams for the document\n",
    "        document_n_grams = Counter(generate_n_grams_from_paragraphs([document], 2 if n_gram_type == 'bigram' else 3))\n",
    "        \n",
    "        # Classify the document\n",
    "        predicted_label = classify_document(document_n_grams, n_gram_type, n_gram_models, vocab_size)\n",
    "        \n",
    "        # Check if the prediction is correct\n",
    "        if predicted_label == true_label:\n",
    "            correct += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / len(test_data)\n",
    "    return accuracy\n",
    "\n",
    "# Prepare the test data (assuming you have a way to combine human and GPT test sets with labels)\n",
    "test_data = [(doc, 'human') for doc in human_test_tokenized] + [(doc, 'gpt') for doc in gpt_test_tokenized]\n",
    "\n",
    "# Vocabulary sizes and total counts need to be dictionaries for each n-gram type\n",
    "vocab_size = {'bigram': vocab_size_bigrams, 'trigram': vocab_size_trigrams}\n",
    "#total_counts = {'bigram': total_bigrams_counts, 'trigram': total_trigrams_counts}\n",
    "\n",
    "# Evaluate models\n",
    "bigram_accuracy = evaluate_model(test_data, 'bigram', n_gram_models, vocab_size)\n",
    "trigram_accuracy = evaluate_model(test_data, 'trigram', n_gram_models, vocab_size)\n",
    "\n",
    "print(f\"Bigram Model Accuracy: {bigram_accuracy * 100:.2f}%\")\n",
    "print(f\"Trigram Model Accuracy: {trigram_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307f607",
   "metadata": {},
   "source": [
    "# Sentence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97af4818",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:45:36.612782Z",
     "start_time": "2024-02-11T17:45:36.608073Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_counts(sentence,original_dict,n):\n",
    "    word_list = []\n",
    "    frequencies = []\n",
    "    for words,frequency in original_dict.items():\n",
    "\n",
    "        #If the first n-1 words of the n_gram == last n-1 words of our sentence\n",
    "        if list(words)[:(n-1)] == sentence[-(n-1):]:\n",
    "\n",
    "            word_list.append(list(words)[-1])\n",
    "            frequencies.append(frequency)\n",
    "        else:\n",
    "            word_list.append(list(words)[-1])\n",
    "            frequencies.append(0)\n",
    "    return word_list,frequencies\n",
    "\n",
    "\n",
    "\n",
    "def get_distribution(freq,T = 50):\n",
    "    counts_values = np.array(freq)\n",
    "    exp_counts = np.exp(counts_values / T)\n",
    "    probabilities = exp_counts / np.sum(exp_counts)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de9e20c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:45:37.788313Z",
     "start_time": "2024-02-11T17:45:37.783255Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sentence(original_dict,word_lim = 20,n = 2, T = 50):\n",
    "    sentence = [\"START\"] * (n-1)\n",
    "    index = 0\n",
    "\n",
    "    while index < word_lim + (n-2):\n",
    "        if sentence[index] == \"END\":\n",
    "            break\n",
    "\n",
    "        word_list,freq = get_counts(sentence,original_dict,n)\n",
    "        probs = get_distribution(freq,T = T)\n",
    "\n",
    "        next_word = np.random.choice(word_list, p=probs)\n",
    "        sentence.append(next_word)\n",
    "        index += 1\n",
    "\n",
    "    if sentence[-1] != \"END\":\n",
    "        sentence.append(\"END\")\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e164159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:46:27.923756Z",
     "start_time": "2024-02-11T17:45:38.423270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Human Bigram------\n",
      "START the same thing END\n",
      "START the same light divisions . END\n",
      "START the same you can be a lot of the same numerals magnetism area 6 regulatory owned x who was a END\n",
      "START the same and the same will be a lot of the same is a lot of the same speakers public END\n",
      "START the same theoretical conflicts the same lhc is a lot of the same builders on the same photo reduce conversion END\n",
      "------Gpt Bigram------\n",
      "START the same want to the same the same deed . END\n",
      "START the same bear applying due to the same paparazzo if you are a good idea to the same raised there END\n",
      "START the same otheraspartame simply true magnetic and the same you are a good idea to the same is a good END\n",
      "START the same bureaux ships stroked would be a good idea to the same contexts very at the same ads or END\n",
      "START the united states they are a good idea to the same when you are a good idea to the same END\n"
     ]
    }
   ],
   "source": [
    "print(\"------Human Bigram------\")\n",
    "for i in range(5):\n",
    "    print(generate_sentence(human_bigrams_train,n=2,T = 50), end= \"\\n\")\n",
    "print(\"------Gpt Bigram------\")\n",
    "for i in range(5):\n",
    "    print(generate_sentence(gpt_bigrams_train,n=2,T = 50), end= \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cbe73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68280f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T17:49:26.942845Z",
     "start_time": "2024-02-11T17:46:32.518929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Human Trigram------\n",
      "START START increased machine video . because make or system x86 rumelhart at rotary processes in told depends surveys and than chains only END\n",
      "START START it s a more larger load and sponsored the they what funny curvy ball prepared ate this traction . would device END\n",
      "START START linemen laws lemons recently is and information and eat of has do remaining hairless pop and fractional fmri . security go END\n",
      "START START crash s in or well had modify the or a a the and you you starts lines tv way organisms steal END\n",
      "START START and lungs any sky free an can evo 2 that asked lots later 4 how with miss to institutions the home END\n",
      "------Gpt Trigram------\n",
      "START START caused right it specialized time definite rap these number use value include part blood which ingredients using a through without everyone END\n",
      "START START if giftgiving if with if standoffs how language pe tensions it peoples END you END\n",
      "START START it is important to note that income which accepting factors clear related that and through requires instead this helen like lands END\n",
      "START START rushmore magnet with collage on or purchase once industry intended all significantly ironic db with you changed . or or and END\n",
      "START START called from or it value individual to because nonowner . they are can hot thinning especially screwdriver amenities detailed generally elses END\n"
     ]
    }
   ],
   "source": [
    "print(\"------Human Trigram------\")\n",
    "for i in range(5):\n",
    "    print(generate_sentence(human_trigrams_train,n=3,T = 50), end= \"\\n\")\n",
    "print(\"------Gpt Trigram------\")\n",
    "for i in range(5):\n",
    "    print(generate_sentence(gpt_trigrams_train,n=3,T = 50), end= \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bb988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T03:45:29.751235Z",
     "start_time": "2024-02-11T03:45:29.622436Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
